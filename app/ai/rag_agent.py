"""
RAG Search Agent - Intelligent context retrieval with query refinement.

Features:
- Query optimization and reformulation
- Relevance scoring with configurable threshold
- Recency weighting for fresher context
- Iterative refinement up to max iterations
- Combined relevance + recency ranking
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from loguru import logger

from pydantic import BaseModel, Field
from langchain_ollama import ChatOllama
from langchain.messages import SystemMessage, HumanMessage


# =============================================================================
# DATA MODELS
# =============================================================================

class QueryPlan(BaseModel):
    """Optimized search queries generated by the agent."""
    queries: List[str] = Field(description="List of search queries to try")
    reasoning: str = Field(description="Why these queries were chosen")


class RelevanceJudgment(BaseModel):
    """Agent's judgment of retrieved context relevance."""
    is_relevant: bool = Field(description="Whether context is relevant to the task")
    relevance_score: float = Field(description="Relevance score 0-1", ge=0, le=1)
    covers_query: bool = Field(description="Whether context fully addresses the query")
    missing_aspects: List[str] = Field(description="What's missing from the context")
    should_continue: bool = Field(description="Whether to search for more context")
    refined_query: Optional[str] = Field(description="Suggested refined query if continuing")


@dataclass
class ScoredChunk:
    """A chunk with combined relevance and recency scores."""
    content: str
    metadata: Dict[str, Any]
    distance: float  # Lower is better (embedding distance)
    relevance_score: float  # Higher is better (0-1)
    recency_score: float  # Higher is better (0-1)
    combined_score: float  # Final weighted score
    source_type: str


@dataclass
class RAGSearchResult:
    """Final result from RAG agent search."""
    context: str
    chunks: List[ScoredChunk]
    queries_used: List[str]
    iterations: int
    final_relevance: float
    total_chunks_found: int
    accepted: bool  # Whether context met threshold


@dataclass
class RAGAgentConfig:
    """Configuration for RAG search agent."""
    max_iterations: int = 10
    relevance_threshold: float = 0.6
    relevance_weight: float = 0.6  # Weight for relevance vs recency
    recency_weight: float = 0.4  # Weight for recency vs relevance
    max_chunks_per_query: int = 10
    max_total_chunks: int = 20
    max_context_chars: int = 4000
    recency_decay_days: int = 30  # Documents older than this get lower recency scores


# =============================================================================
# RAG SEARCH AGENT
# =============================================================================

class RAGSearchAgent:
    """
    Agent that intelligently searches RAG with query refinement.
    
    Workflow:
    1. Analyze original query and generate optimized search queries
    2. Execute searches and score results by relevance + recency
    3. Judge if retrieved context is sufficient
    4. If not, refine query and iterate (up to max_iterations)
    5. Return best context found
    """
    
    def __init__(
        self,
        rag,  # ContextRAG instance
        llm_model: str = "qwen3:30b",
        base_url: str = "http://100.91.155.118:11434",
        config: Optional[RAGAgentConfig] = None,
    ):
        self.rag = rag
        self.config = config or RAGAgentConfig()
        self.llm = ChatOllama(
            model=llm_model,
            temperature=0,
            base_url=base_url,
        )
        self._all_chunks: Dict[str, ScoredChunk] = {}  # Dedupe by content hash
    
    async def search(
        self,
        query: str,
        task_context: str = "",
        workflow_id: Optional[str] = None,
        on_progress: Optional[callable] = None,
    ) -> RAGSearchResult:
        """
        Search RAG with intelligent query refinement.
        
        Args:
            query: User's original query
            task_context: Additional context about the task
            workflow_id: Limit search to specific workflow
            on_progress: Callback for progress updates
        """
        self._all_chunks.clear()
        queries_used = []
        
        # Generate initial search queries
        query_plan = self._generate_queries(query, task_context)
        current_queries = query_plan.queries[:3]  # Start with top 3
        
        if on_progress:
            on_progress(f"Generated {len(query_plan.queries)} search queries")
        
        for iteration in range(self.config.max_iterations):
            if on_progress:
                on_progress(f"Search iteration {iteration + 1}/{self.config.max_iterations}")
            
            # Execute searches
            for q in current_queries:
                if q not in queries_used:
                    queries_used.append(q)
                    chunks = self._execute_search(q, workflow_id)
                    
                    if on_progress and chunks:
                        on_progress(f"Found {len(chunks)} chunks for: {q[:50]}...")
            
            # Get best chunks so far
            ranked_chunks = self._get_ranked_chunks()
            
            if not ranked_chunks:
                # No results, try broader queries
                current_queries = self._broaden_queries(query, queries_used)
                continue
            
            # Build context from top chunks
            context = self._build_context(ranked_chunks)
            
            # Judge relevance
            judgment = self._judge_relevance(query, task_context, context)
            
            if on_progress:
                on_progress(f"Relevance: {judgment.relevance_score:.2f}, covers query: {judgment.covers_query}")
            
            # Check if we should stop
            if judgment.relevance_score >= self.config.relevance_threshold and judgment.covers_query:
                logger.info(f"✅ RAG search complete: {len(ranked_chunks)} chunks, relevance {judgment.relevance_score:.2f}")
                return RAGSearchResult(
                    context=context,
                    chunks=ranked_chunks[:self.config.max_total_chunks],
                    queries_used=queries_used,
                    iterations=iteration + 1,
                    final_relevance=judgment.relevance_score,
                    total_chunks_found=len(self._all_chunks),
                    accepted=True,
                )
            
            # Check if we should continue
            if not judgment.should_continue:
                break
            
            # Refine query for next iteration
            if judgment.refined_query:
                current_queries = [judgment.refined_query]
            else:
                current_queries = self._refine_queries(
                    query, 
                    judgment.missing_aspects,
                    queries_used
                )
        
        # Return best effort result
        ranked_chunks = self._get_ranked_chunks()
        context = self._build_context(ranked_chunks) if ranked_chunks else ""
        final_relevance = self._quick_relevance_score(query, context) if context else 0.0
        
        accepted = final_relevance >= self.config.relevance_threshold
        
        logger.info(f"{'✅' if accepted else '⚠️'} RAG search finished: {len(ranked_chunks)} chunks, relevance {final_relevance:.2f}")
        
        return RAGSearchResult(
            context=context,
            chunks=ranked_chunks[:self.config.max_total_chunks],
            queries_used=queries_used,
            iterations=self.config.max_iterations,
            final_relevance=final_relevance,
            total_chunks_found=len(self._all_chunks),
            accepted=accepted,
        )
    
    def _generate_queries(self, query: str, task_context: str) -> QueryPlan:
        """Generate optimized search queries from original query."""
        prompt = f"""Generate search queries to find relevant context in a RAG database.

ORIGINAL QUERY:
{query}

TASK CONTEXT:
{task_context or "General data analysis task"}

The RAG contains:
- Previous analysis results and summaries
- Code execution outputs
- Web search results about methodology
- Uploaded documents

Generate 3-5 diverse search queries that would find relevant information.
Include:
- Direct keyword queries
- Conceptual/semantic queries  
- Queries for related methodology or techniques

Return queries ordered by expected relevance."""

        try:
            structured_llm = self.llm.with_structured_output(QueryPlan)
            result = structured_llm.invoke([
                SystemMessage(content="Generate effective RAG search queries."),
                HumanMessage(content=prompt),
            ])
            logger.debug(f"Generated queries: {result.queries}")
            return result
        except Exception as e:
            logger.warning(f"Query generation failed: {e}")
            # Fallback: extract key terms
            return QueryPlan(
                queries=[query, query.split()[0] if query.split() else query],
                reasoning="Fallback to original query"
            )
    
    def _execute_search(
        self, 
        query: str, 
        workflow_id: Optional[str]
    ) -> List[ScoredChunk]:
        """Execute a single search and score results."""
        if not self.rag or not self.rag.enabled:
            return []
        
        try:
            results = self.rag.query_relevant_context(
                query=query,
                workflow_id=workflow_id,
                n_results=self.config.max_chunks_per_query,
            )
            
            scored = []
            for r in results:
                chunk = self._score_chunk(r, query)
                
                # Dedupe by content
                content_hash = hash(chunk.content[:200])
                if content_hash not in self._all_chunks:
                    self._all_chunks[content_hash] = chunk
                    scored.append(chunk)
            
            return scored
            
        except Exception as e:
            logger.warning(f"RAG search failed for '{query[:30]}...': {e}")
            return []
    
    def _score_chunk(self, result: Dict[str, Any], query: str) -> ScoredChunk:
        """Score a chunk by relevance and recency."""
        content = result.get("document", "")
        metadata = result.get("metadata", {})
        distance = result.get("distance", 1.0)
        
        # Relevance score: convert distance to 0-1 (lower distance = higher relevance)
        # Typical distances range 0.2-1.5, normalize to 0-1
        relevance_score = max(0, min(1, 1 - (distance / 1.5)))
        
        # Recency score: based on timestamp
        recency_score = self._calculate_recency_score(metadata)
        
        # Combined score with configured weights
        combined_score = (
            self.config.relevance_weight * relevance_score +
            self.config.recency_weight * recency_score
        )
        
        return ScoredChunk(
            content=content,
            metadata=metadata,
            distance=distance,
            relevance_score=relevance_score,
            recency_score=recency_score,
            combined_score=combined_score,
            source_type=metadata.get("type", "unknown"),
        )
    
    def _calculate_recency_score(self, metadata: Dict[str, Any]) -> float:
        """Calculate recency score from metadata timestamp."""
        timestamp_str = metadata.get("timestamp")
        if not timestamp_str:
            return 0.5  # Default for unknown age
        
        try:
            # Parse ISO timestamp
            if isinstance(timestamp_str, str):
                timestamp = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
            else:
                timestamp = timestamp_str
            
            # Calculate age in days
            now = datetime.now(timestamp.tzinfo) if timestamp.tzinfo else datetime.now()
            age_days = (now - timestamp).total_seconds() / 86400
            
            # Exponential decay: score = e^(-age/decay_constant)
            decay = self.config.recency_decay_days
            recency_score = max(0, min(1, 1 - (age_days / (decay * 2))))
            
            return recency_score
            
        except Exception as e:
            logger.debug(f"Could not parse timestamp: {e}")
            return 0.5
    
    def _get_ranked_chunks(self) -> List[ScoredChunk]:
        """Get all chunks ranked by combined score."""
        chunks = list(self._all_chunks.values())
        chunks.sort(key=lambda c: c.combined_score, reverse=True)
        return chunks
    
    def _build_context(self, chunks: List[ScoredChunk]) -> str:
        """Build context string from ranked chunks."""
        if not chunks:
            return ""
        
        parts = []
        total_chars = 0
        max_chars = self.config.max_context_chars
        
        # Group by source type for organization
        by_type: Dict[str, List[ScoredChunk]] = {}
        for chunk in chunks[:self.config.max_total_chunks]:
            source = chunk.source_type
            if source not in by_type:
                by_type[source] = []
            by_type[source].append(chunk)
        
        # Build context with type headers
        for source_type, type_chunks in by_type.items():
            if total_chars >= max_chars:
                break
            
            header = f"\n[{source_type.upper()}]"
            parts.append(header)
            total_chars += len(header)
            
            for chunk in type_chunks:
                if total_chars >= max_chars:
                    break
                
                content = chunk.content
                remaining = max_chars - total_chars
                
                if len(content) > remaining:
                    content = content[:remaining - 3] + "..."
                
                parts.append(content)
                total_chars += len(content)
        
        return "\n".join(parts)
    
    def _judge_relevance(
        self, 
        query: str, 
        task_context: str, 
        context: str
    ) -> RelevanceJudgment:
        """Judge if retrieved context is relevant and sufficient."""
        prompt = f"""Evaluate if this retrieved context is relevant and sufficient for the task.

ORIGINAL QUERY:
{query}

TASK CONTEXT:
{task_context or "Data analysis task"}

RETRIEVED CONTEXT:
{context[:3000]}

Evaluate:
1. Is this context relevant to the query? (relevance_score 0-1)
2. Does it fully address what's needed? (covers_query)
3. What aspects are missing? (missing_aspects)
4. Should we search for more context? (should_continue)
5. If continuing, what refined query would help? (refined_query)

Be critical - only mark covers_query=true if the context genuinely addresses the query."""

        try:
            structured_llm = self.llm.with_structured_output(RelevanceJudgment)
            return structured_llm.invoke([
                SystemMessage(content="Evaluate RAG context relevance critically."),
                HumanMessage(content=prompt),
            ])
        except Exception as e:
            logger.warning(f"Relevance judgment failed: {e}")
            return RelevanceJudgment(
                is_relevant=bool(context),
                relevance_score=0.5 if context else 0.0,
                covers_query=False,
                missing_aspects=["Could not evaluate"],
                should_continue=True,
                refined_query=None,
            )
    
    def _quick_relevance_score(self, query: str, context: str) -> float:
        """Quick relevance score without full LLM judgment."""
        if not context:
            return 0.0
        
        # Simple keyword overlap score
        query_words = set(query.lower().split())
        context_words = set(context.lower().split())
        
        if not query_words:
            return 0.5
        
        overlap = len(query_words & context_words)
        score = overlap / len(query_words)
        
        return min(1.0, score)
    
    def _refine_queries(
        self, 
        original_query: str,
        missing_aspects: List[str],
        used_queries: List[str]
    ) -> List[str]:
        """Generate refined queries based on what's missing."""
        if not missing_aspects:
            return []
        
        prompt = f"""Generate new search queries to find missing information.

ORIGINAL QUERY:
{original_query}

MISSING ASPECTS:
{', '.join(missing_aspects)}

ALREADY TRIED:
{', '.join(used_queries[-5:])}

Generate 2-3 new queries targeting the missing aspects.
Make them different from queries already tried."""

        try:
            structured_llm = self.llm.with_structured_output(QueryPlan)
            result = structured_llm.invoke([
                SystemMessage(content="Generate targeted search queries for missing information."),
                HumanMessage(content=prompt),
            ])
            return result.queries[:3]
        except Exception as e:
            logger.warning(f"Query refinement failed: {e}")
            return [missing_aspects[0]] if missing_aspects else []
    
    def _broaden_queries(
        self, 
        original_query: str,
        used_queries: List[str]
    ) -> List[str]:
        """Generate broader queries when no results found."""
        # Extract key nouns/concepts
        words = original_query.split()
        
        broader = []
        if len(words) > 2:
            # Try first half, second half
            mid = len(words) // 2
            broader.append(" ".join(words[:mid]))
            broader.append(" ".join(words[mid:]))
        
        # Try individual significant words (longer than 4 chars)
        for word in words:
            if len(word) > 4 and word.lower() not in used_queries:
                broader.append(word)
        
        return broader[:3]


# =============================================================================
# CONVENIENCE FUNCTION
# =============================================================================

async def search_rag_with_agent(
    query: str,
    rag,
    task_context: str = "",
    workflow_id: Optional[str] = None,
    llm_model: str = "qwen3:30b",
    base_url: str = "http://100.91.155.118:11434",
    config: Optional[RAGAgentConfig] = None,
    on_progress: Optional[callable] = None,
) -> RAGSearchResult:
    """
    Convenience function to search RAG with agent.
    
    Args:
        query: Search query
        rag: ContextRAG instance
        task_context: Additional context about the task
        workflow_id: Limit to specific workflow
        llm_model: Model for query generation/judgment
        base_url: Ollama base URL
        config: Agent configuration
        on_progress: Progress callback
    """
    agent = RAGSearchAgent(
        rag=rag,
        llm_model=llm_model,
        base_url=base_url,
        config=config,
    )
    
    return await agent.search(
        query=query,
        task_context=task_context,
        workflow_id=workflow_id,
        on_progress=on_progress,
    )